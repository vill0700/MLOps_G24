{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"entry_point/","title":"Entry point of app through the fronten","text":""},{"location":"entry_point/#frontend","title":"Frontend","text":""},{"location":"entry_point/#mlopsg24.frontend","title":"mlopsg24.frontend","text":""},{"location":"entry_point/#mlopsg24.frontend.get_localhost_api_client","title":"get_localhost_api_client","text":"<pre><code>get_localhost_api_client()\n</code></pre> <p>Create and cache the TestClient instance</p> Source code in <code>mlopsg24/frontend.py</code> <pre><code>@st.cache_resource\ndef get_localhost_api_client():\n    \"\"\"Create and cache the TestClient instance\"\"\"\n\n    client = TestClient(app)\n\n    client.__enter__()  # Triggers lifespan startup (loads model)\n\n    def cleanup():\n        try:\n            client.__exit__(None, None, None)  # Triggers lifespan shutdown\n        except Exception as e:\n            st.warning(f\"Error during client cleanup: {e}\")\n\n    atexit.register(cleanup)\n\n    return client\n</code></pre>"},{"location":"entry_point/#api","title":"API","text":""},{"location":"entry_point/#mlopsg24.api","title":"mlopsg24.api","text":""},{"location":"entry_point/#mlopsg24.api.add_to_database","title":"add_to_database","text":"<pre><code>add_to_database(\n    dataclass_prediction: DataPrediction, jobopslag: str\n)\n</code></pre> <p>Add record to databas of predictions</p> Source code in <code>mlopsg24/api.py</code> <pre><code>def add_to_database(dataclass_prediction:DataPrediction, jobopslag:str):\n    \"\"\"Add record to databas of predictions\"\"\"\n    # NOTE: Simple example of a database record\n    # For proper setup it should be record to a DB table and\n    # contain input or ID to input jobopslag instead of non-unique clean_str\n    now = str(datetime.now())\n    clean_str = re.sub(r'[^a-zA-Z ]', '', jobopslag)\n    path_mock_database = Path(\"data/drift/prediction_records.csv\")\n\n    # Create directory if it doesn't exist. Otherwise github workflow test can fail\n    path_mock_database.parent.mkdir(parents=True, exist_ok=True)\n\n    with path_mock_database.open(\"a\", encoding=\"utf-8\") as file:\n        file.write(f\"{now}, {clean_str[:100]}, {dataclass_prediction.categori_label}, {dataclass_prediction.categori_idx}\\n\")\n</code></pre>"},{"location":"entry_point/#mlopsg24.api.health_check","title":"health_check","text":"<pre><code>health_check()\n</code></pre> <p>Health check.</p> Source code in <code>mlopsg24/api.py</code> <pre><code>@app.get(\"/\")\ndef health_check():\n    \"\"\"Health check.\"\"\"\n    response = {\n        \"message\": HTTPStatus.OK.phrase,\n        \"status-code\": HTTPStatus.OK,\n    }\n    return response\n</code></pre>"},{"location":"entry_point/#mlopsg24.api.levetid","title":"levetid  <code>async</code>","text":"<pre><code>levetid(app: FastAPI)\n</code></pre> <ul> <li>Loads a pretrained hugginface model into FastAPI once at first call.</li> <li>At shutdown of API, the hf model is deleted and memory is released.</li> </ul> Source code in <code>mlopsg24/api.py</code> <pre><code>@asynccontextmanager\nasync def levetid(app: FastAPI):\n    \"\"\"\n    - Loads a pretrained hugginface model into FastAPI once at first call.\n    - At shutdown of API, the hf model is deleted and memory is released.\n    \"\"\"\n    app.state.inferencer = InferenceClassify()\n    logger.info(\"instance of InferenceClassify() loaded into FastAPI app.state\")\n\n    yield\n\n    del app.state.inferencer\n    gc.collect()\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n    logger.info(\"succesfully closed. Deleted instance of InferenceClassify(). Cleared GPU - just in case\")\n</code></pre>"},{"location":"entry_point/#mlopsg24.api.predict","title":"predict  <code>async</code>","text":"<pre><code>predict(\n    jobopslag: str, background_task: BackgroundTasks\n) -&gt; dict\n</code></pre> <p>Makes a prediction of type DataPrediction. Inserts a record into a database to enable monitoring of data drift. Return a json/dict to the user</p> Source code in <code>mlopsg24/api.py</code> <pre><code>@app.post(\"/classify\")\nasync def predict(jobopslag: str, background_task:BackgroundTasks) -&gt; dict:\n    \"\"\"\n    Makes a prediction of type DataPrediction.\n    Inserts a record into a database to enable monitoring of data drift.\n    Return a json/dict to the user\n    \"\"\"\n\n    dataclass_prediction = app.state.inferencer.classify(jobopslag)\n\n    background_task.add_task(add_to_database, dataclass_prediction, jobopslag)\n\n    return asdict(dataclass_prediction)\n</code></pre>"},{"location":"entry_point/#mlopsg24.api.predict_batch","title":"predict_batch  <code>async</code>","text":"<pre><code>predict_batch(\n    list_jobopslag: List[str],\n    background_task: BackgroundTasks,\n) -&gt; dict[int, dict]\n</code></pre> <p>Batch version of predict(). Take a list of jobopslag</p> Source code in <code>mlopsg24/api.py</code> <pre><code>@app.post(\"/batch_classify\")\nasync def predict_batch(list_jobopslag: List[str], background_task:BackgroundTasks) -&gt; dict[int, dict]:\n    \"\"\"\n    Batch version of predict().\n    Take a list of jobopslag\n    \"\"\"\n    #NOTE: not truely a batch - for batch to work it must be refactored in\n    # inferencer and add_to_database as a bulk insert (eg PGSQL COPY BINARY)\n\n    json_results = {}\n    for idx,jobopslag in enumerate(list_jobopslag):\n        dataclass_prediction = app.state.inferencer.classify(jobopslag)\n        background_task.add_task(add_to_database, dataclass_prediction, jobopslag)\n        json_results[idx] = asdict(dataclass_prediction)\n\n    return json_results\n</code></pre>"},{"location":"entry_point/#inference","title":"Inference","text":""},{"location":"entry_point/#mlopsg24.inference","title":"mlopsg24.inference","text":""},{"location":"entry_point/#mlopsg24.inference.InferenceClassify","title":"InferenceClassify","text":"<p>This is meant as a inference pipeline, that processes a single datapoint ie. a Danish job vacancy. Sets up a instance to be run using method 'classify()'. The imported and used modules are from a batch / DataFrame style pipeline</p> Source code in <code>mlopsg24/inference.py</code> <pre><code>class InferenceClassify:\n    \"\"\"\n    This is meant as a inference pipeline, that processes a single datapoint\n    ie. a Danish job vacancy.\n    Sets up a instance to be run using method 'classify()'.\n    The imported and used modules are from a batch / DataFrame style pipeline\n    \"\"\"\n\n    def __init__(\n        self,\n        name_model_gliner2: str = \"fastino/gliner2-multi-v1\",  # NOTE: belongs in config\n    ) -&gt; None:\n\n        # Use local saved model if it exists, otherwise download from Huggingface ID\n        self.path_local_gliner2 = Path(\"models\" / Path(name_model_gliner2))\n\n        self.path_gliner2: str = (\n            str(self.path_local_gliner2)\n            if self.path_local_gliner2.exists()\n            else name_model_gliner2\n        )\n\n        # Load HuggingFace text model\n        self.model_extractor = GLiNER2.from_pretrained(self.path_gliner2)\n\n        # load text embedding model on CPU for inference\n        self.model_preprocesser = PreprocessData()\n        self.model_preprocesser.init_text_embedder(gpu=False)\n\n        # load inference map\n        dim_idx = pl.read_parquet(Path(\"data/processed/category_mapping.parquet\"))\n        self.dict_idx_category = dict(zip(dim_idx[\"idx\"], dim_idx[\"categori\"]))\n\n        # load trained classifier to eval mode and cpu\n        self.model_classifier = NeuralNetwork()\n        state_dict = torch.load(DEFAULT_OUTPUT, map_location=\"cpu\")[\"state_dict\"]\n        self.model_classifier.load_state_dict(state_dict)\n        self.model_classifier.eval()\n\n        logger.info(\n            \"\\nLoaded ML models should all be on cpu:\"\n            f\"\\n{self.model_extractor.device = }\"\n            f\"\\n{self.model_preprocesser.text_embedder.device = }\"\n            f\"\\n{next(self.model_classifier.parameters()).device = }\"\n        )\n\n    def classify(self, jobopslag_text: str) -&gt; DataPrediction:\n        \"\"\"\n        Main function meant to be called.\n\n        Pipeline\n        ---\n        1) augments by extracting stillingsbetegnelse, kompetencer and\n        arbejdsopgaver from a job vacancy text, then rewrites to a single string.\n        2) text embeds the augmented text\n        3) runs trained classifier using the embedding as input features\n\n        Args\n        ---\n        jobopslag text\n\n        Returns\n        ---\n        Dataclass containing predictions etc.\n        \"\"\"\n        text_augmented = augment_jobopslag_text(text=jobopslag_text, model_gliner2=self.model_extractor)\n\n        embedding = self.model_preprocesser.text_embedder.encode(\n            sentences=text_augmented,\n            convert_to_tensor=True,\n            show_progress_bar=False,\n            prompt=self.model_preprocesser.embedding_prefix,\n            normalize_embeddings=True,\n        )\n\n        message = None\n        if len(text_augmented) &lt; 10:\n            # message to be passed on to API and Frontend\n            message = (\n                \"Der kunne ikke tr\u00e6kkes stillingsbetegnelse, kompetencer eller \"\n                \"arbejdsopgaver ud af det jobopslag du har indtastet. \"\n                \"Pr\u00f8v igen med mere beskrivende jobopslag. \"\n                f\"ERROR: {text_augmented = }\"\n            )\n            logger.error(message)\n\n        # disable gradient for inference speed - see lecture notes\n        with torch.no_grad():\n            nn_output = self.model_classifier(embedding)\n\n        predicted_class_idx = torch.argmax(nn_output).item()\n\n        probabilities = F.softmax(nn_output, dim=0).detach().cpu().tolist()\n\n        return DataPrediction(\n            categori_label=self.dict_idx_category[predicted_class_idx],\n            categori_idx=predicted_class_idx,\n            probability_distribution=probabilities,\n            frontend_error_message=message,\n        )\n</code></pre>"},{"location":"entry_point/#mlopsg24.inference.InferenceClassify.classify","title":"classify","text":"<pre><code>classify(jobopslag_text: str) -&gt; DataPrediction\n</code></pre> <p>Main function meant to be called.</p>"},{"location":"entry_point/#mlopsg24.inference.InferenceClassify.classify--pipeline","title":"Pipeline","text":"<p>1) augments by extracting stillingsbetegnelse, kompetencer and arbejdsopgaver from a job vacancy text, then rewrites to a single string. 2) text embeds the augmented text 3) runs trained classifier using the embedding as input features</p>"},{"location":"entry_point/#mlopsg24.inference.InferenceClassify.classify--args","title":"Args","text":"<p>jobopslag text</p>"},{"location":"entry_point/#mlopsg24.inference.InferenceClassify.classify--returns","title":"Returns","text":"<p>Dataclass containing predictions etc.</p> Source code in <code>mlopsg24/inference.py</code> <pre><code>def classify(self, jobopslag_text: str) -&gt; DataPrediction:\n    \"\"\"\n    Main function meant to be called.\n\n    Pipeline\n    ---\n    1) augments by extracting stillingsbetegnelse, kompetencer and\n    arbejdsopgaver from a job vacancy text, then rewrites to a single string.\n    2) text embeds the augmented text\n    3) runs trained classifier using the embedding as input features\n\n    Args\n    ---\n    jobopslag text\n\n    Returns\n    ---\n    Dataclass containing predictions etc.\n    \"\"\"\n    text_augmented = augment_jobopslag_text(text=jobopslag_text, model_gliner2=self.model_extractor)\n\n    embedding = self.model_preprocesser.text_embedder.encode(\n        sentences=text_augmented,\n        convert_to_tensor=True,\n        show_progress_bar=False,\n        prompt=self.model_preprocesser.embedding_prefix,\n        normalize_embeddings=True,\n    )\n\n    message = None\n    if len(text_augmented) &lt; 10:\n        # message to be passed on to API and Frontend\n        message = (\n            \"Der kunne ikke tr\u00e6kkes stillingsbetegnelse, kompetencer eller \"\n            \"arbejdsopgaver ud af det jobopslag du har indtastet. \"\n            \"Pr\u00f8v igen med mere beskrivende jobopslag. \"\n            f\"ERROR: {text_augmented = }\"\n        )\n        logger.error(message)\n\n    # disable gradient for inference speed - see lecture notes\n    with torch.no_grad():\n        nn_output = self.model_classifier(embedding)\n\n    predicted_class_idx = torch.argmax(nn_output).item()\n\n    probabilities = F.softmax(nn_output, dim=0).detach().cpu().tolist()\n\n    return DataPrediction(\n        categori_label=self.dict_idx_category[predicted_class_idx],\n        categori_idx=predicted_class_idx,\n        probability_distribution=probabilities,\n        frontend_error_message=message,\n    )\n</code></pre>"},{"location":"monitoring/","title":"Monitoring","text":"<p>Here you can see how to run the monitoring of the data drifts</p>"},{"location":"monitoring/#mlopsg24.data_drift","title":"mlopsg24.data_drift","text":""},{"location":"monitoring/#mlopsg24.data_drift.monitor_drift_embeddings","title":"monitor_drift_embeddings","text":"<pre><code>monitor_drift_embeddings() -&gt; None\n</code></pre>"},{"location":"monitoring/#mlopsg24.data_drift.monitor_drift_embeddings--what-function-does","title":"What function does","text":"<p>This is a demonstration of monitoring data drift in text embeddings.</p> <p>The function compares the embeddings in the test data to the validation.</p> <p>This mimics a data drift monotoring, where the test data is used as reference data and validation data is used as the data being monitored for data drift.</p>"},{"location":"monitoring/#mlopsg24.data_drift.monitor_drift_embeddings--how-and-why","title":"How and why","text":"<ol> <li>Convert the .pt files to pandas</li> <li>Takes a subset for demonstration - otherwise it is too slow.</li> <li>Use one df columns for schema - it would be an error if the columns are not identical</li> <li>Saves a html report in /reports/monitoring/</li> </ol> Source code in <code>mlopsg24/data_drift.py</code> <pre><code>def monitor_drift_embeddings() -&gt; None:\n    \"\"\"\n    What function does\n    -----\n    This is a demonstration of monitoring data drift in text embeddings.\n\n    The function compares the embeddings in the test data to the validation.\n\n    This mimics a data drift monotoring, where the test data is used as\n    reference data and validation data is used as the data being monitored for\n    data drift.\n\n    How and why\n    -----\n    1. Convert the .pt files to pandas\n    2. Takes a subset for demonstration - otherwise it is too slow.\n    3. Use one df columns for schema - it would be an error if the columns are not identical\n    4. Saves a html report in /reports/monitoring/\n    \"\"\"\n    ts_x_test = torch.load('data/processed/x_test.pt').detach().cpu().numpy()\n    df_x_test = pd.DataFrame(data=ts_x_test, columns=[f\"col_{i}\" for i in range(ts_x_test.shape[1])])\n    df_x_test_subset = df_x_test.iloc[:1000,:100]\n\n    ts_x_val = torch.load('data/processed/x_val.pt').detach().cpu().numpy()\n    df_x_val = pd.DataFrame(data=ts_x_val, columns=[f\"col_{i}\" for i in range(ts_x_val.shape[1])])\n    df_x_val_subset = df_x_val.iloc[:1000,:100]\n\n    schema = DataDefinition(numerical_columns=list(df_x_val_subset.columns))\n\n    logger.info(\"generating report drift_training_jobopslag.html\")\n    (\n        Report([DataDriftPreset(embeddings=list(df_x_val_subset.columns))])\n        .run(\n            reference_data = (\n                Dataset.from_pandas(\n                    data = df_x_test_subset,\n                    data_definition = schema,\n                )),\n            current_data=(\n                Dataset.from_pandas(\n                    data = df_x_val_subset,\n                    data_definition = schema,\n                )),\n        )\n        .save_html(\"reports/monitoring/drift_embeddings.html\")\n    )\n</code></pre>"},{"location":"monitoring/#mlopsg24.data_drift.monitor_drift_training_jobopslag","title":"monitor_drift_training_jobopslag","text":"<pre><code>monitor_drift_training_jobopslag() -&gt; None\n</code></pre>"},{"location":"monitoring/#mlopsg24.data_drift.monitor_drift_training_jobopslag--what-function-does","title":"What function does","text":"<p>This is a demonstration of monitoring data drift in categorical data</p> <p>The function compares categorical data from year 2022 to 2024.</p> <p>This mimics a data drift monotoring, where the 2022 data is used as reference data and 2024 data is used as the data being monitored for data drift.</p> <p>The function outputs 3 reports /reports/monitoring/</p> Source code in <code>mlopsg24/data_drift.py</code> <pre><code>def monitor_drift_training_jobopslag() -&gt; None:\n    \"\"\"\n    What function does\n    -----\n    This is a demonstration of monitoring data drift in categorical data\n\n    The function compares categorical data from year 2022 to 2024.\n\n    This mimics a data drift monotoring, where the 2022 data is used as\n    reference data and 2024 data is used as the data being monitored for\n    data drift.\n\n    The function outputs 3 reports /reports/monitoring/\n    \"\"\"\n\n    df_raw = pl.read_parquet(\"data/raw/training_jobopslag.parquet\")\n\n    columns_to_inspect = ['label','erhvervsgruppe_txt','erhvervsomraade_txt',]\n\n    schema = DataDefinition(categorical_columns=columns_to_inspect)\n\n    df_2022 = Dataset.from_pandas(\n        data = (\n            df_raw\n            .filter(pl.col('startdt').dt.year()==\n                2022\n            )\n            .select(columns_to_inspect)\n            .to_pandas()\n        ),\n        data_definition=schema,\n    )\n\n    df_2024 = Dataset.from_pandas(\n        data = df_raw\n            .filter(pl.col('startdt').dt.year()==\n                2024\n            )\n            .select(columns_to_inspect)\n            .to_pandas(),\n        data_definition=schema,\n    )\n\n    logger.info(\"generating report summary_training_jobopslag.html\")\n    (\n        Report([DataSummaryPreset()])\n        .run(reference_data=df_2022, current_data=df_2024)\n        .save_html(\"reports/monitoring/summary_training_jobopslag.html\")\n    )\n\n    logger.info(\"generating report stats_training_jobopslag.html\")\n    (\n        Report([DatasetStats()])\n        .run(reference_data=df_2022, current_data=df_2024)\n        .save_html(\"reports/monitoring/stats_training_jobopslag.html\")\n    )\n\n    logger.info(\"generating report drift_training_jobopslag.html\")\n    (\n        Report([DataDriftPreset(drift_share=0.1)])\n        .run(reference_data=df_2022, current_data=df_2024)\n        .save_html(\"reports/monitoring/drift_training_jobopslag.html\")\n    )\n</code></pre>"},{"location":"preprocess/","title":"Preprocess","text":""},{"location":"preprocess/#mlopsg24.data_preprocess.PreprocessData","title":"mlopsg24.data_preprocess.PreprocessData","text":"<p>Preprocessing pipeline for job vacancy text classification.</p> <p>Handles loading raw job posting data, creating text embeddings using SentenceTransformer, preparing categorical targets, and splitting data into train/validation/test sets for model training.</p> <p>Attributes:</p> Name Type Description <code>path_text_embedder</code> <code>str</code> <p>Path to the SentenceTransformer model</p> <code>file_data_raw</code> <p>Path to raw parquet data file</p> <code>test_size</code> <p>Proportion of data for test set (default 0.2)</p> <code>val_size</code> <p>Proportion of data for validation set (default 0.1)</p> <code>random_state</code> <p>Random seed for reproducibility (default 42)</p> <code>batch_size</code> <p>Batch size for embedding generation (default 32)</p> <code>path_output</code> <p>Output directory for processed data (default \"data/processed\")</p> <code>column_target_class</code> <p>Target column name (default \"erhvervsomraade_txt\")</p> <code>column_text</code> <p>Text column name for embeddings (default \"annonce_tekst\")</p> <code>embedding_prefix</code> <p>Instruction prefix for text embedding model</p> Source code in <code>mlopsg24/data_preprocess.py</code> <pre><code>class PreprocessData:\n    \"\"\"\n    Preprocessing pipeline for job vacancy text classification.\n\n    Handles loading raw job posting data, creating text embeddings using\n    SentenceTransformer, preparing categorical targets, and splitting data\n    into train/validation/test sets for model training.\n\n    Attributes:\n        path_text_embedder: Path to the SentenceTransformer model\n        file_data_raw: Path to raw parquet data file\n        test_size: Proportion of data for test set (default 0.2)\n        val_size: Proportion of data for validation set (default 0.1)\n        random_state: Random seed for reproducibility (default 42)\n        batch_size: Batch size for embedding generation (default 32)\n        path_output: Output directory for processed data (default \"data/processed\")\n        column_target_class: Target column name (default \"erhvervsomraade_txt\")\n        column_text: Text column name for embeddings (default \"annonce_tekst\")\n        embedding_prefix: Instruction prefix for text embedding model\n    \"\"\"\n    def __init__(\n        self,\n        name_text_embedder: str = \"intfloat/multilingual-e5-large-instruct\",\n        file_data_raw: Path = Path(\"data/raw/training_jobopslag.parquet\"),\n        test_size: float = 0.2,\n        val_size: float = 0.1,\n        random_state: int = 42,\n        batch_size: int = 32,\n        path_output: Path = Path(\"data/processed\"),\n        column_target_class: str = \"erhvervsomraade_txt\",\n        column_text: str = \"annonce_tekst\",\n        embedding_prefix: str = (\n            \"query: \"\n            \"Classify the following extracted texts of occupation, skills \"\n            \"and tasks from a Danish job vacancy into job category\"\n        ),\n    ) -&gt; None:\n        self.file_data_raw = file_data_raw\n        self.test_size = test_size\n        self.val_size = val_size\n        self.random_state = random_state\n        self.path_output = path_output\n        self.column_target_class = column_target_class\n        self.column_text = column_text\n        self.embedding_prefix = embedding_prefix\n        self.batch_size = batch_size\n\n        # Use local saved model if exists, otherwise try downloading from HuggingFace ID\n        self.path_local_text_embedder =  Path(\"models\" / Path(name_text_embedder))\n\n        self.path_text_embedder: str = (\n            str(self.path_local_text_embedder)\n            if self.path_local_text_embedder.exists()\n            else name_text_embedder\n        )\n\n        if not self.path_output.exists():\n            logger.info(f\"Creating data processed output directory: {self.path_output}\")\n            self.path_output.mkdir(parents=True, exist_ok=True)\n\n    def extract_input_data(self) -&gt; None:\n        \"\"\"\n        Mount raw data to dataframe\n        \"\"\"\n\n        logger.info(f\"Reading data from {self.file_data_raw}\")\n        self.df_jobopslag = pl.read_parquet(self.file_data_raw)\n\n    def init_text_embedder(self, gpu: bool = True):\n        \"\"\"\n        Initialize SentenceTransformer model for text embeddings.\n        Default tries to use GPU\n        \"\"\"\n        logger.info(f\"Loading text embedder from {self.path_text_embedder}\")\n\n        self.text_embedder = SentenceTransformer(\n            model_name_or_path=str(self.path_text_embedder),\n            device=\"cuda\" if (torch.cuda.is_available() and gpu) else \"cpu\",\n        )\n\n    def create_x_features(self):\n        \"\"\"\n        Create text embeddings used as x features.\n        Processes in batches to manage GPU memory efficiently.\n        \"\"\"\n\n        list_sentences = self.df_jobopslag.select(self.column_text).to_series().to_list()\n\n        logger.info(f\"Creating embeddings for {len(list_sentences)} observations in batches of {self.batch_size}\")\n\n        # Initialize list to store embeddings from each batch\n        all_embeddings = []\n\n        # Calculate number of batches\n        num_batches = (len(list_sentences) + self.batch_size - 1) // self.batch_size\n\n        # Process in batches with progress bar\n        for i in tqdm(range(0, len(list_sentences), self.batch_size), desc=\"Creating embeddings\", total=num_batches):\n            # Get current batch\n            batch_sentences = list_sentences[i : i + self.batch_size]\n\n            # Generate embeddings for batch\n            batch_embeddings = self.text_embedder.encode(\n                sentences=batch_sentences,\n                convert_to_tensor=True,\n                show_progress_bar=False,\n                prompt=self.embedding_prefix,\n                normalize_embeddings=True,\n            )\n\n            # Move to CPU and convert to numpy to free GPU memory\n            batch_embeddings_cpu = batch_embeddings.cpu().numpy()\n            all_embeddings.append(batch_embeddings_cpu)\n\n            # Clear GPU cache\n            if torch.cuda.is_available():\n                del batch_embeddings\n                torch.cuda.empty_cache()\n\n        # Concatenate all batches and convert back to tensor\n        self.x_features = torch.from_numpy(np.vstack(all_embeddings))\n\n        logger.info(f\"x features embeddings shape: {self.x_features.shape}\")\n\n    def create_y_target(self):\n        \"\"\"\n        Create categorical y target features\n        Saves mapping of y idx to classes to parquet table\n        \"\"\"\n\n        y_categories = self.df_jobopslag.select(self.column_target_class).to_series()\n\n        # Create mapping from categories to indices\n        unique_categories = y_categories.unique().sort()\n        category_to_idx = {cat: idx for idx, cat in enumerate(unique_categories)}\n\n        # Convert to tensor\n        self.y_targets = torch.tensor([category_to_idx[cat] for cat in y_categories.to_list()], dtype=torch.long)\n\n        # save mapping of categories\n        (\n            pl.DataFrame(\n                data=list(category_to_idx.items()),\n                schema=(\"categori\", \"idx\"),\n                orient=\"row\",\n            ).write_parquet(self.path_output / \"category_mapping.parquet\")\n        )\n\n        logger.info(f\"Targets shape: {self.y_targets.shape}\")\n        logger.info(f\"Number of classes: {len(unique_categories)}\")\n\n\n    def split_data(self) -&gt; None:\n        \"\"\"\n        Split data into train, validation, and test sets.\n        \"\"\"\n        logger.info(\"Splitting data into train/val/test sets\")\n\n        x_temp, self.x_test, y_temp, self.y_test = train_test_split(\n            self.x_features,\n            self.y_targets,\n            test_size=self.test_size,\n            random_state=self.random_state,\n            stratify=self.y_targets,  # Maintains class distribution\n        )\n\n        # Second split: separate validation from training\n        # Adjust val_size relative to temp size to ensure equal distribution\n        val_size_adjusted = self.val_size / (1 - self.test_size)\n        self.x_train, self.x_val, self.y_train, self.y_val = train_test_split(\n            x_temp, y_temp, test_size=val_size_adjusted, random_state=self.random_state, stratify=y_temp\n        )\n\n        logger.info(f\"Train set: {self.x_train.shape[0]} samples\")\n        logger.info(f\"Validation set: {self.x_val.shape[0]} samples\")\n        logger.info(f\"Test set: {self.x_test.shape[0]} samples\")\n\n    def save_data(self) -&gt; None:\n        \"\"\"\n        Save train,test,validation for x and y as tensors to .pt files.\n        \"\"\"\n        logger.info(\"Saving data\")\n\n        torch.save(self.x_train, self.path_output / \"x_train.pt\")\n        torch.save(self.x_val, self.path_output / \"x_val.pt\")\n        torch.save(self.x_test, self.path_output / \"x_test.pt\")\n        torch.save(self.y_train, self.path_output / \"y_train.pt\")\n        torch.save(self.y_val, self.path_output / \"y_val.pt\")\n        torch.save(self.y_test, self.path_output / \"y_test.pt\")\n\n        logger.info(f\"All tensors saved successfully to {self.path_output}\")\n\n    def main(self):\n        \"\"\"\n        Main preprocessing pipeline.\n        \"\"\"\n        logger.info(\"Starting preprocessing pipeline\")\n\n        self.extract_input_data()\n        self.init_text_embedder()\n        self.create_x_features()\n        self.create_y_target()\n        self.split_data()\n        self.save_data()\n\n        logger.info(\"Preprocessing complete with train/val/test split\")\n</code></pre>"},{"location":"preprocess/#mlopsg24.data_preprocess.PreprocessData.create_x_features","title":"create_x_features","text":"<pre><code>create_x_features()\n</code></pre> <p>Create text embeddings used as x features. Processes in batches to manage GPU memory efficiently.</p> Source code in <code>mlopsg24/data_preprocess.py</code> <pre><code>def create_x_features(self):\n    \"\"\"\n    Create text embeddings used as x features.\n    Processes in batches to manage GPU memory efficiently.\n    \"\"\"\n\n    list_sentences = self.df_jobopslag.select(self.column_text).to_series().to_list()\n\n    logger.info(f\"Creating embeddings for {len(list_sentences)} observations in batches of {self.batch_size}\")\n\n    # Initialize list to store embeddings from each batch\n    all_embeddings = []\n\n    # Calculate number of batches\n    num_batches = (len(list_sentences) + self.batch_size - 1) // self.batch_size\n\n    # Process in batches with progress bar\n    for i in tqdm(range(0, len(list_sentences), self.batch_size), desc=\"Creating embeddings\", total=num_batches):\n        # Get current batch\n        batch_sentences = list_sentences[i : i + self.batch_size]\n\n        # Generate embeddings for batch\n        batch_embeddings = self.text_embedder.encode(\n            sentences=batch_sentences,\n            convert_to_tensor=True,\n            show_progress_bar=False,\n            prompt=self.embedding_prefix,\n            normalize_embeddings=True,\n        )\n\n        # Move to CPU and convert to numpy to free GPU memory\n        batch_embeddings_cpu = batch_embeddings.cpu().numpy()\n        all_embeddings.append(batch_embeddings_cpu)\n\n        # Clear GPU cache\n        if torch.cuda.is_available():\n            del batch_embeddings\n            torch.cuda.empty_cache()\n\n    # Concatenate all batches and convert back to tensor\n    self.x_features = torch.from_numpy(np.vstack(all_embeddings))\n\n    logger.info(f\"x features embeddings shape: {self.x_features.shape}\")\n</code></pre>"},{"location":"preprocess/#mlopsg24.data_preprocess.PreprocessData.create_y_target","title":"create_y_target","text":"<pre><code>create_y_target()\n</code></pre> <p>Create categorical y target features Saves mapping of y idx to classes to parquet table</p> Source code in <code>mlopsg24/data_preprocess.py</code> <pre><code>def create_y_target(self):\n    \"\"\"\n    Create categorical y target features\n    Saves mapping of y idx to classes to parquet table\n    \"\"\"\n\n    y_categories = self.df_jobopslag.select(self.column_target_class).to_series()\n\n    # Create mapping from categories to indices\n    unique_categories = y_categories.unique().sort()\n    category_to_idx = {cat: idx for idx, cat in enumerate(unique_categories)}\n\n    # Convert to tensor\n    self.y_targets = torch.tensor([category_to_idx[cat] for cat in y_categories.to_list()], dtype=torch.long)\n\n    # save mapping of categories\n    (\n        pl.DataFrame(\n            data=list(category_to_idx.items()),\n            schema=(\"categori\", \"idx\"),\n            orient=\"row\",\n        ).write_parquet(self.path_output / \"category_mapping.parquet\")\n    )\n\n    logger.info(f\"Targets shape: {self.y_targets.shape}\")\n    logger.info(f\"Number of classes: {len(unique_categories)}\")\n</code></pre>"},{"location":"preprocess/#mlopsg24.data_preprocess.PreprocessData.extract_input_data","title":"extract_input_data","text":"<pre><code>extract_input_data() -&gt; None\n</code></pre> <p>Mount raw data to dataframe</p> Source code in <code>mlopsg24/data_preprocess.py</code> <pre><code>def extract_input_data(self) -&gt; None:\n    \"\"\"\n    Mount raw data to dataframe\n    \"\"\"\n\n    logger.info(f\"Reading data from {self.file_data_raw}\")\n    self.df_jobopslag = pl.read_parquet(self.file_data_raw)\n</code></pre>"},{"location":"preprocess/#mlopsg24.data_preprocess.PreprocessData.init_text_embedder","title":"init_text_embedder","text":"<pre><code>init_text_embedder(gpu: bool = True)\n</code></pre> <p>Initialize SentenceTransformer model for text embeddings. Default tries to use GPU</p> Source code in <code>mlopsg24/data_preprocess.py</code> <pre><code>def init_text_embedder(self, gpu: bool = True):\n    \"\"\"\n    Initialize SentenceTransformer model for text embeddings.\n    Default tries to use GPU\n    \"\"\"\n    logger.info(f\"Loading text embedder from {self.path_text_embedder}\")\n\n    self.text_embedder = SentenceTransformer(\n        model_name_or_path=str(self.path_text_embedder),\n        device=\"cuda\" if (torch.cuda.is_available() and gpu) else \"cpu\",\n    )\n</code></pre>"},{"location":"preprocess/#mlopsg24.data_preprocess.PreprocessData.main","title":"main","text":"<pre><code>main()\n</code></pre> <p>Main preprocessing pipeline.</p> Source code in <code>mlopsg24/data_preprocess.py</code> <pre><code>def main(self):\n    \"\"\"\n    Main preprocessing pipeline.\n    \"\"\"\n    logger.info(\"Starting preprocessing pipeline\")\n\n    self.extract_input_data()\n    self.init_text_embedder()\n    self.create_x_features()\n    self.create_y_target()\n    self.split_data()\n    self.save_data()\n\n    logger.info(\"Preprocessing complete with train/val/test split\")\n</code></pre>"},{"location":"preprocess/#mlopsg24.data_preprocess.PreprocessData.save_data","title":"save_data","text":"<pre><code>save_data() -&gt; None\n</code></pre> <p>Save train,test,validation for x and y as tensors to .pt files.</p> Source code in <code>mlopsg24/data_preprocess.py</code> <pre><code>def save_data(self) -&gt; None:\n    \"\"\"\n    Save train,test,validation for x and y as tensors to .pt files.\n    \"\"\"\n    logger.info(\"Saving data\")\n\n    torch.save(self.x_train, self.path_output / \"x_train.pt\")\n    torch.save(self.x_val, self.path_output / \"x_val.pt\")\n    torch.save(self.x_test, self.path_output / \"x_test.pt\")\n    torch.save(self.y_train, self.path_output / \"y_train.pt\")\n    torch.save(self.y_val, self.path_output / \"y_val.pt\")\n    torch.save(self.y_test, self.path_output / \"y_test.pt\")\n\n    logger.info(f\"All tensors saved successfully to {self.path_output}\")\n</code></pre>"},{"location":"preprocess/#mlopsg24.data_preprocess.PreprocessData.split_data","title":"split_data","text":"<pre><code>split_data() -&gt; None\n</code></pre> <p>Split data into train, validation, and test sets.</p> Source code in <code>mlopsg24/data_preprocess.py</code> <pre><code>def split_data(self) -&gt; None:\n    \"\"\"\n    Split data into train, validation, and test sets.\n    \"\"\"\n    logger.info(\"Splitting data into train/val/test sets\")\n\n    x_temp, self.x_test, y_temp, self.y_test = train_test_split(\n        self.x_features,\n        self.y_targets,\n        test_size=self.test_size,\n        random_state=self.random_state,\n        stratify=self.y_targets,  # Maintains class distribution\n    )\n\n    # Second split: separate validation from training\n    # Adjust val_size relative to temp size to ensure equal distribution\n    val_size_adjusted = self.val_size / (1 - self.test_size)\n    self.x_train, self.x_val, self.y_train, self.y_val = train_test_split(\n        x_temp, y_temp, test_size=val_size_adjusted, random_state=self.random_state, stratify=y_temp\n    )\n\n    logger.info(f\"Train set: {self.x_train.shape[0]} samples\")\n    logger.info(f\"Validation set: {self.x_val.shape[0]} samples\")\n    logger.info(f\"Test set: {self.x_test.shape[0]} samples\")\n</code></pre>"},{"location":"symbolic_link_to_readme/","title":"Home","text":""},{"location":"symbolic_link_to_readme/#ml-ops-project","title":"ML Ops Project","text":"<p>Our main objective with this project is to create a machine learning process that extracts data from Jobnet, transforms it, and uses it to train a model to match an external job vacancies to one of Jobnet's predefined job categories.</p> <p>The framework of the project is as follows: 1. Data Extraction: From a private database we extract job vacancies from Jobnet. Data is tabular with fields of  the job vacancy text itself and 22 classes of job type. Job vacancy texts contain personal information and noisy text not relevant to the job type, which is cleaned using GLINER2 configure to only extracts occupation, skills and task. These are then rewritten to a string to be more compatible with a standard SentenceTransformer embedding model. This data is made available to the entire group. 2. Data Transformation:     The extracted data text are transformed into a text embedding using SentenceTransformer.     Returns transformed data of labelled classes to vector embeddings of a pytorch tensor datatype of floats.     The model chosen is sentence-transformers/paraphrase-multilingual-mpnet-base-v2, which is an older but standard multilingual text embedding model. For further performance could be considered, models such as intfloat/multilingual-e5-large-instruct can be used - which requires a prefix, or finetuning a text embedding model with SentenceTransformerTrainer using GenAI to judge triplet textdata of anchor: posive,negative data. 3. Model Training: We use the transformed data to train a machine learning model. It is simply a logistic regression that takes the 1024 dim embeddings and outputs logits for the 22 classes. The models can be quantized to int8 weights and pruned for the 20% smallest weights. 4. Model Evaluation: After training, we evaluate the model's performance using accuracy. We use hold-one-out cross validation, and furthermore split the training set into a training and validation set. We also make confusion matrices for the models. 5. Deployment: Finally, we deploy the trained model to a production environment where it can be used to classify new job vacancies and match them to Jobnet's categories.# mlopsg24</p> <p>jobannonce classifier</p>"},{"location":"symbolic_link_to_readme/#architectual-overview-of-inference-and-retraining-pipeline","title":"Architectual overview of inference and retraining pipeline","text":""},{"location":"symbolic_link_to_readme/#project-structure","title":"Project structure","text":"<p>The directory structure of the project looks like this:</p> <pre><code>\u251c\u2500\u2500 .github/                  # Github actions and dependabot\n\u2502   \u251c\u2500\u2500 dependabot.yaml\n\u2502   \u2514\u2500\u2500 workflows/\n\u2502       \u251c\u2500\u2500 codecheck_format  # makes a basic ruff check\n\u2502       \u251c\u2500\u2500 deploy_docs       # deploys a mkdocs to github\n\u2502       \u2514\u2500\u2500 tests.yaml        # unit and integraiton tests\n\u251c\u2500\u2500 configs/                  # N/A Configuration files N/A\n\u251c\u2500\u2500 data/                     # Data directory\n\u2502   \u251c\u2500\u2500 processed             # texts processed to tensors and split into train/test/val\n\u2502   \u251c\u2500\u2500 drift                 # \"database\" of for data drift monitoring\n\u2502   \u2514\u2500\u2500 raw                   # Text data from jobopslag\n\u251c\u2500\u2500 dockerfiles/              # Dockerfiles\n\u2502   \u251c\u2500\u2500 api.Dockerfile\n\u2502   \u2514\u2500\u2500 train.Dockerfile\n\u251c\u2500\u2500 docs/                     # Documentation\n\u2502   \u251c\u2500\u2500 images                # images for markdowns\n\u2502   \u251c\u2500\u2500 mkdocs.yml            # builds a mkdocs\n\u2502   \u2514\u2500\u2500 source/               # md files for mkdocs\n\u251c\u2500\u2500 models/                   # Trained model weigths, and pretrained Hugginface models\n\u251c\u2500\u2500 notebooks/                # N/A Jupyter notebooks\n\u251c\u2500\u2500 reports/                  # Reports\n\u2502   \u251c\u2500\u2500 monitoring/           # data drift monitoring reports in html\n\u2502   \u251c\u2500\u2500 figures/\n\u2502   \u2514\u2500\u2500 data_create_html      # jupyter interactive output of data_create.py\n\u251c\u2500\u2500 src/                      # Source code\n\u2502   \u251c\u2500\u2500 mlopsg24/\n\u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u251c\u2500\u2500 api.py            # FastAPI endpoint\n\u2502   \u2502   \u251c\u2500\u2500 data_create.py    # extracts data from a private db - cannot be run\n\u2502   \u2502   \u251c\u2500\u2500 data_drift.py # monitoring data drift reports\n\u2502   \u2502   \u251c\u2500\u2500 data_preproces.py # module to embed texts\n\u2502   \u2502   \u251c\u2500\u2500 frontend.py       # streamlit frontend\n\u2502   \u2502   \u251c\u2500\u2500 inference.py      # inference pipeline on a single data point\n\u2502   \u2502   \u251c\u2500\u2500 model.py         # the ANN\n|   |   \u251c\u2500\u2500 modified_model_timer.py # times the standard, quantized, pruned and quant+pruned models\n\u2502   \u2502   \u251c\u2500\u2500 train.py          # training pipeline\n\u2502   \u2502   \u2514\u2500\u2500 visualize.py      # N/A\n\u2514\u2500\u2500 tests/\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 unittests.py\n\u2502       \u2514\u2500\u2500 test_data_preprocess.py   # example of unit test on preprocess pipeline\n\u2502   \u2514\u2500\u2500 unittests.py\n\u2502       \u2514\u2500\u2500 test_api.py       # example of integration test on the api\n\u251c\u2500\u2500 .gitignore\n\u251c\u2500\u2500 .pre-commit-config.yaml   # standard precommits and a local safetymeasure against pushing datafiles\n\u251c\u2500\u2500 LICENSE\n\u251c\u2500\u2500 pyproject.toml            # Python project file\n\u251c\u2500\u2500 README.md                 # Project README\n\u251c\u2500\u2500 requirements.txt          # N/A Project requirements\n\u251c\u2500\u2500 requirements_dev.txt      # N/A Development requirements\n\u2514\u2500\u2500 tasks.py                  # CLI Project tasks using invoke\n</code></pre>"}]}